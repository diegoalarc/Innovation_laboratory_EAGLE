# [Innovation_laboratory_EAGLE](http://eagle-science.org/project/tmt1-innovation-laboratory/)
The innovation laboratory provides the opportunity to conduct a specified research project on a chosen research topic and explore the potential, challenges and limits of Earth Observation and geoanalysis in a practical approach during the 3rd term.
The Innovation Laboratory allows you to analyze one particular topic in your field of interest in depth. It allows to address own research in the field of the study program and offers the basis of practical attained knowledge. It is similar to an internship but allows to do it at the university (esp. our department).

---

## Content
The content of the innovation laboratory can be decided by each student individually and either a research topic is offered by a lecturer or the student is proposing an own topic. Research topics need to be discussed with and proposed to one EAGLE lecturer who will also be in charge of supervising and grading the students work. Topics of the innovation laboratory can cover all aspects of the EAGLE study program with a strong focus on applied Earth Observation and geoanalysis or its innovative potential for remote sensing sciences. It may comprise topics such as linking spectrometer field studies to remotely sensed data or the exploration of UAV based imagery, as well as space borne earth observation analysis such as time-series derivation for a variety of environmental studies i.e. resource mapping or spatial predictions and statistics of variables.

---

## Topic:
### Machine Learning process for yield prediction for the "Carmen Rosa" farm located in Chile.

Project carried out with the help of the company Carmen Rosa, who has provided data, both for its weather station and its annual production, in order to carry out an analysis and possible production optimization considering:

- Temperature
- Evapotranspiration
- Humidity
- Annual production
- Shapefile with the precise location of the crops

Image of Carmen Rosa Farm:
![Carmen Rosa farm location](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Carmen_Rosa_Farm.png?raw=true "Carmen Rosa farm location")

#### For better precision, check the location by downloading the following kmz:
 - [Kmz of Carmen Rosa Farm](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/raw/main/Original_data/CARMEN_ROSA_farm.kmz)

---

## Google Earth Engine
To carry out this project, an algorithm was generated through Google Earth Engine to obtain the images between November and December from 2016 to 2019, which represent the most important phenological stage for these grapes in this area of Chile. In the algorithm the images of GNDVI, NDVI and EVI were generated, which give us information about the crops and their health. Within the same algorithm, a mean of all the images for all their bands was obtained, which were downloaded and processed by the algorithms generated in R.
To obtain the information for the prediction of the year 2021, this algorithm was also used to obtain the desired images and bands.

The codes are as follows:
[Sentinel-2](https://code.earthengine.google.com/10bef8017a4fcacec6ef47296e2b9018)

---

## R Code generated
Then a code was applied to obtain a unique value per field for each band generated by the Google Earth Engine code and for each year, which will be applied in a machine learning model.

The codes are as follows:
[R code](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/NDVI_GNDVI_EVI_cellStats_by_Field.R)

---

## R Code boxplot
In order to get a general idea of each field, an NDVI analysis was performed for each field. For this, an analysis was performed through a boxplot of each NDVI image of each field and a point was placed that represents the mean of the total value of pixels of each image for each year.

The codes are as follows:
[R Boxplot code](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Boxplot.R)

#### Considerations when reading the graph:
For the fields 'INIAGRAPE ONE 15' and 'SABLE 16', only the years 2018 - 2019 are valid, since they did not present production in the years 2016 - 2017. This data was eliminated from the dataframe to create the machine learning model.

#### Boxplot:
![Example of NDVI for each field from the years 2016 -2019](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/NDVI_2017_to_2020.png?raw=true "Example of NDVI for each field from the years 2016 -2019")

## R Code [Caret Package](https://topepo.github.io/caret/index.html)
Applying the [Caret](https://topepo.github.io/caret/index.html) de R package, a [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning) model was made to make a prediction of the Yield of Carmen Rosa farm.
According to what has been done, the need for more data to make a prediction with better precision is evident. But considering the lack of these, it was decided to make (anyway) an [ML](https://en.wikipedia.org/wiki/Machine_learning) model, considering that it will improve over the years as more information is available.

### Evaluation and selection of Hyperparameters

When analyzing the values for tuning in the [Random Forest](https://en.wikipedia.org/wiki/Random_forest#:~:text=Random%20forests%20or%20random%20decision,average%20prediction%20(regression)%20of%20the) model and in the Conditional Random Forest model, the Caret package was used, which provides an analysis of the values and search for the best one for the model evaluation and for a better application. results.

Random Forest hyperparameters:
```r
44 samples
42 predictors

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 40, 38, 40, 41, 39, 40, ... 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE     
    1   11369.28  0.5825408  8833.336
    2   11187.04  0.5510065  8548.529
    3   11214.11  0.5542960  8518.524
    4   11280.64  0.5260950  8576.022
    5   11454.10  0.5108149  8730.777
    6   11326.53  0.4984254  8587.088
    7   11390.08  0.4687324  8618.069
    8   11372.20  0.4830655  8636.803
    9   11367.76  0.4972114  8599.217
   10   11454.36  0.4841415  8704.176
   11   11487.35  0.4623855  8676.495
   12   11489.73  0.4420062  8736.847
   13   11431.55  0.5210702  8625.165
   14   11455.65  0.4963291  8690.094
   15   11423.21  0.4045614  8666.902
   16   11491.77  0.4621505  8705.965
   17   11404.79  0.4887218  8631.536
   18   11498.22  0.4868000  8674.489
   19   11512.13  0.4452073  8717.027
   20   11489.83  0.5126499  8698.679
   21   11523.79  0.4246943  8723.497
   22   11571.69  0.4868538  8732.012
   23   11655.51  0.4804124  8828.010
   24   11624.01  0.4772529  8770.177
   25   11458.02  0.4705264  8660.399
   26   11543.06  0.4792360  8727.092
   27   11577.93  0.3981259  8745.861
   28   11502.48  0.4536830  8745.675
   29   11653.73  0.4615854  8844.886
   30   11582.39  0.4797681  8795.526
   31   11612.45  0.4819642  8806.489
   32   11557.24  0.4455155  8752.635
   33   11601.80  0.4377148  8815.541
   34   11574.44  0.4659301  8786.594
   35   11509.75  0.4672181  8779.044
   36   11557.63  0.4135373  8855.100
   37   11625.95  0.4740759  8861.364
   38   11609.74  0.4743826  8854.181
   39   11638.52  0.4577677  8869.886
   40   11708.63  0.4242821  8906.295
   41   11635.22  0.3848762  8894.682
   42   11553.82  0.4022087  8811.883
   43   11686.58  0.4682168  8949.433
   44   11636.14  0.4539872  8908.966
   45   11641.17  0.4153014  8910.314
   46   11559.50  0.4265850  8811.665
   47   11611.56  0.3723212  8888.233
   48   11657.92  0.4586258  8919.970
   49   11601.11  0.4747952  8921.727
   50   11692.68  0.4246420  8870.220
   51   11722.24  0.4615710  8965.082
   52   11595.01  0.4182075  8855.852
   53   11651.09  0.4599959  8917.252
   54   11698.82  0.4673639  8928.388
   55   11656.28  0.4545950  8873.276
   56   11653.82  0.4743123  8927.030
   57   11631.57  0.4095302  8871.065
   58   11531.90  0.4754550  8811.641
   59   11596.96  0.4545041  8874.456
   60   11567.83  0.4224442  8883.209
   61   11642.80  0.3879275  8844.138
   62   11548.03  0.4693166  8790.038
   63   11628.16  0.4057807  8897.744
   64   11685.79  0.4621325  8909.707
   65   11721.85  0.3640896  8969.716
   66   11666.48  0.3650941  8901.387
   67   11654.76  0.4164440  8864.363
   68   11626.54  0.4664820  8913.165
   69   11708.78  0.4026191  8917.122
   70   11474.42  0.4842566  8739.101
   71   11654.67  0.4660331  8914.949
   72   11624.39  0.3651094  8885.744
   73   11596.51  0.4282103  8870.106
   74   11666.24  0.4602137  8948.716
   75   11674.80  0.4529241  8944.255
   76   11656.06  0.4528167  8978.879
   77   11672.22  0.4538665  8905.883
   78   11596.49  0.4602130  8847.518
   79   11586.99  0.4199313  8852.786
   80   11618.05  0.4474265  8896.157
   81   11620.57  0.3837569  8898.779
   82   11631.53  0.4634065  8870.268
   83   11660.31  0.4644471  8944.814
   84   11579.20  0.4679330  8810.191
   85   11687.77  0.3844401  8939.302
   86   11653.16  0.4742313  8890.548
   87   11750.52  0.4738149  8952.295
   88   11624.19  0.4559587  8903.530
   89   11737.18  0.3699152  8986.237
   90   11500.70  0.4629959  8775.226
   91   11654.46  0.4041018  8866.241
   92   11677.77  0.3762777  8902.974
   93   11628.18  0.4515944  8869.444
   94   11506.91  0.4797659  8755.749
   95   11728.14  0.4670378  8997.887
   96   11669.60  0.4421655  8916.967
   97   11454.02  0.4404753  8750.353
   98   11537.45  0.4305736  8862.995
   99   11649.92  0.4395591  8882.417
  100   11643.44  0.4552979  8893.972

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 2.

predictions_rf
       2        6        7       15       16       23       24       27       31       35 
37616.31 50451.14 22783.34 48717.02 47673.42 32221.37 31292.14 48348.13 33664.34 37137.60 
      38       39 
51574.15 44328.54
```
![RMS vs Ramdom Predictors Random Forest model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/RMSE_vs_Ramdom_Predictors_rforest.png?raw=true "RMS vs Ramdom Predictors Random Forest model")


Conditional Random Forest hyperparameters:
```r
44 samples
42 predictors

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 40, 38, 40, 41, 39, 40, ... 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE      
    1   12333.24  0.3853406  10111.060
    2   12030.29  0.5129408   9787.982
    3   11904.45  0.4989630   9580.630
    4   11872.74  0.4241099   9521.488
    5   11753.34  0.4691263   9477.053
    6   11800.63  0.4938131   9492.160
    7   11831.63  0.4793971   9506.669
    8   11709.38  0.4603914   9338.788
    9   11675.17  0.5167871   9351.079
   10   11676.07  0.5036210   9315.453
   11   11697.41  0.4597627   9299.707
   12   11733.96  0.4928548   9381.092
   13   11660.43  0.4896101   9253.773
   14   11581.98  0.5010718   9213.499
   15   11690.30  0.5081050   9308.830
   16   11670.20  0.4960640   9254.243
   17   11761.05  0.4614463   9337.341
   18   11689.77  0.5230617   9287.445
   19   11651.37  0.5055056   9237.189
   20   11636.51  0.5077867   9298.295
   21   11684.60  0.4919418   9334.129
   22   11648.78  0.5052838   9276.429
   23   11708.27  0.4878141   9305.966
   24   11633.00  0.4897809   9255.390
   25   11658.06  0.5091878   9263.946
   26   11615.39  0.5141192   9193.376
   27   11611.76  0.5009969   9201.247
   28   11621.92  0.4970718   9234.868
   29   11673.25  0.4971165   9248.417
   30   11767.66  0.4856989   9345.296
   31   11692.40  0.4785830   9225.647
   32   11637.38  0.5048787   9245.169
   33   11695.11  0.4867293   9318.479
   34   11653.33  0.5034803   9250.589
   35   11583.65  0.5239194   9196.512
   36   11764.16  0.4683458   9330.326
   37   11656.26  0.4750987   9218.673
   38   11708.21  0.4859665   9285.849
   39   11683.69  0.4845550   9260.369
   40   11700.15  0.4999535   9248.788
   41   11694.07  0.5018198   9285.181
   42   11700.72  0.4956080   9259.620
   43   11653.32  0.4837239   9215.420
   44   11687.09  0.4850622   9209.130
   45   11715.59  0.4883332   9278.797
   46   11673.57  0.4896086   9241.786
   47   11767.08  0.4722506   9318.222
   48   11746.48  0.4660950   9306.346
   49   11687.00  0.4895300   9251.456
   50   11673.40  0.5051879   9260.188
   51   11687.76  0.5011283   9229.653
   52   11705.64  0.4989726   9275.069
   53   11731.64  0.4654349   9257.461
   54   11650.37  0.4972405   9236.948
   55   11735.22  0.4817002   9278.994
   56   11829.21  0.4727345   9351.931
   57   11674.67  0.4949034   9249.404
   58   11735.97  0.4575142   9264.450
   59   11710.86  0.4902270   9260.930
   60   11731.73  0.4772106   9321.768
   61   11639.57  0.5035347   9219.993
   62   11679.72  0.4911197   9263.066
   63   11704.58  0.4819921   9237.230
   64   11664.74  0.4848711   9279.650
   65   11701.36  0.4925179   9320.576
   66   11699.67  0.4640449   9286.086
   67   11742.16  0.4749673   9299.569
   68   11701.38  0.4908444   9239.432
   69   11699.83  0.4689559   9284.701
   70   11694.55  0.4822459   9278.303
   71   11828.78  0.4605363   9398.057
   72   11702.53  0.4733471   9267.024
   73   11574.26  0.4950811   9160.008
   74   11696.70  0.5013582   9314.596
   75   11784.63  0.4652983   9363.763
   76   11726.53  0.5028747   9334.742
   77   11627.18  0.5160994   9206.861
   78   11643.85  0.4926419   9213.662
   79   11707.46  0.4888041   9282.897
   80   11698.36  0.4750101   9276.367
   81   11672.63  0.4818484   9218.622
   82   11622.99  0.4867739   9223.885
   83   11638.13  0.5055147   9218.348
   84   11670.46  0.4922934   9235.342
   85   11711.65  0.5025058   9258.032
   86   11691.13  0.4992768   9306.685
   87   11671.32  0.4941279   9258.745
   88   11686.36  0.4920082   9246.084
   89   11773.15  0.4512359   9353.516
   90   11740.93  0.4817501   9273.856
   91   11663.84  0.4881041   9230.773
   92   11647.48  0.4871584   9194.183
   93   11733.20  0.4793747   9273.111
   94   11644.36  0.4901615   9207.097
   95   11619.07  0.5045112   9181.930
   96   11663.32  0.4874072   9245.649
   97   11696.10  0.4769912   9279.328
   98   11665.48  0.5131101   9206.781
   99   11760.26  0.4669882   9308.116
  100   11708.61  0.4789429   9257.197

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 73.

predictions_crf
 [1] 36882.24 48567.60 32606.83 49857.94 49750.02 36080.43 36333.06 40673.48 36623.21 36909.67
[11] 49896.75 41456.92
```
![RMS vs Ramdom Predictors Conditional Random Forest model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/RMSE_vs_Ramdom_Predictors_cforest.png?raw=true "RMS vs Ramdom Predictors Conditional Random Forest model")

### Plot of variable importance
Something important when looking at the generated model, is variable importance, which is possible thanks to Caret's "varImp" function.

- RandomForest model:
![Variable importance ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_rforest_ggplo2.png?raw=true "Variable importance ML model")

- Conditional RandomForest model:
![Variable importance ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_cforest_ggplo2.png?raw=true "Variable importance ML model")

### Values of R2, RMSE & MAE
These are the values with the best performance, after the auto-selection of the tuning through the use of the Caret package.

- RandomForest model:

| `R2` | `RMSE Kg` | `MAE Kg` |
| :-------: | :------: | :-----: |
| 0.9434978 | 3039.821 | 2328.789 |

|`RMSE without dimension`|
| :--------------------: |
| 0.07480269 |

- Conditional RandomForest model:

| `R2` | `RMSE Kg` | `MAE Kg` |
| :-------: | :------: | :-----: |
| 0.7291815 | 6439.277 | 5277.034 |

|`RMSE without dimension`|
| :--------------------: |
| 0.1584551 |

### Model comparison

To make this comparison, two functions of the Caret package were used:

1. A resamples of the train generated by each model was carried out.
2. A boxplot of these was generated by analyzing their Rsquared.

- Rsquared's Bloxplot:

![Rsquared's Bloxplot](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/RF_vs_cRF_Boxplot_Rsquared_ggplot.png?raw=true "Rsquared's Bloxplot")