!["Uni Wuerzburg"](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/EAGLE_logo.png?raw=true "EAGLE Msc")

[![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)

# [Innovation laboratory EAGLE](http://eagle-science.org/project/tmt1-innovation-laboratory/)
The innovation laboratory provides the opportunity to conduct a specified research project on a chosen research topic and explore the potential, challenges and limits of Earth Observation and geoanalysis in a practical approach during the 3rd term.
The Innovation Laboratory allows you to analyze one particular topic in your field of interest in depth. It allows to address own research in the field of the study program and offers the basis of practical attained knowledge. It is similar to an internship but allows to do it at the university (esp. our department).

---

## Content
The content of the innovation laboratory can be decided by each student individually and either a research topic is offered by a lecturer or the student is proposing an own topic. Research topics need to be discussed with and proposed to one EAGLE lecturer who will also be in charge of supervising and grading the students work. Topics of the innovation laboratory can cover all aspects of the EAGLE study program with a strong focus on applied Earth Observation and geoanalysis or its innovative potential for remote sensing sciences. It may comprise topics such as linking spectrometer field studies to remotely sensed data or the exploration of UAV based imagery, as well as space borne earth observation analysis such as time-series derivation for a variety of environmental studies i.e. resource mapping or spatial predictions and statistics of variables.

---

## Topic:
### Machine Learning process for yield prediction for the "Carmen Rosa" farm located in Chile.

Project carried out with the help of the company Carmen Rosa, who has provided data, both for its weather station and its annual production, in order to carry out an analysis and possible production optimization considering:

- Temperature
- Evapotranspiration
- Humidity
- Fields area
- Grade day
- Annual production
- KMZ (converted to Shapefile) with the precise location of the crops

Image of Carmen Rosa Farm:
![Carmen Rosa farm location](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Carmen_Rosa_Farm.png?raw=true "Carmen Rosa farm location")

#### For better precision, check the location by downloading the following kmz:
 - [Kmz of Carmen Rosa Farm](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/raw/main/Original_data/CARMEN_ROSA_farm.kmz)

---

## Google Earth Engine
To carry out this project, an algorithm was generated through Google Earth Engine to obtain the images between November and December from 2016 to 2019, which represent the most important phenological stage for these grapes in this area of Chile. Then a Python code was applied to convert a collection from TOA to BOA. These corrected images were in turn saved in an image collection on my personal account. This new collection was cut to reduce the bands to the region of interest and then by means of a Google Earth Engine code, the GNDVI, NDVI and EVI indices obtained for the desired phenological stage were generated, which give us information about the crops and their health. Within the same algorithm, a mean of all the images for all their bands was obtained, which were downloaded and processed by the algorithms generated in R.
To obtain the information for the prediction of the year 2021, this algorithm was also used.

The codes are as follows:

[Python code](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/tree/main/Python_code)

[Sentinel-2](https://code.earthengine.google.com/?scriptPath=users%2Fdiegoalarcondiaz%2FThesis_test%3ASentinel_2_GNDVI_NDVI_EVI_BOA)

The indices calculated through GEE were:

![NDVI](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/NDVI.svg?raw=true "NDVI")

![GNDVI](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/GNDVI.svg?raw=true "GNDVI")


![EVI](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/EVI.svg?raw=true "EVI")

---

## R Code generated
Then a code was applied to obtain a unique value per field for each band generated by the Google Earth Engine code and for each year, only with the data obtained for the desired phenological stage, which will be applied in a machine learning model.

The codes are as follows:
[R code](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/NDVI_GNDVI_EVI_cellStats_by_Field.R)

---

## R Code boxplot
In order to get a general idea of each field, an NDVI analysis was performed for each field. For this, an analysis was performed through a boxplot of each NDVI image of each field and a point was placed that represents the mean of the total value of pixels of each image for each year.

The codes are as follows:
[R Boxplot code](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Boxplot.R)

##### Considerations when reading the graph:
```r
For the fields 'INIAGRAPE ONE 15' and 'SABLE 16', only the years 2018 to 2020 are valid.
The other data was removed from the data frame to create the machine learning model.
```

#### Boxplot:
![Example of NDVI for each field from the years 2016 -2020](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/NDVI_2017_to_2021.png?raw=true "Example of NDVI for each field from the years 2016 -2020")

---

## R Code [Caret Package](https://topepo.github.io/caret/index.html)
Applying the [Caret](https://topepo.github.io/caret/index.html) de R package, a [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning) model was made to make a prediction of the Yield of Carmen Rosa farm.
According to what has been done, the need for more data to make a prediction with better precision is evident. But considering the lack of these, it was decided to make (anyway) an [ML](https://en.wikipedia.org/wiki/Machine_learning) model, considering that it will improve over the years as more information is available.

### Evaluation and selection of Hyperparameters

When analyzing the values for tuning in the [Random Forest](https://en.wikipedia.org/wiki/Random_forest#:~:text=Random%20forests%20or%20random%20decision,average%20prediction%20(regression)%20of%20the) model and in the [Conditional Random Forest model](https://www.tandfonline.com/doi/abs/10.1198/106186006X133933), the Caret package was used, which provides an analysis of the values and search for the best one for the model evaluation and for a better application. results.

Random Forest hyperparameters:
```r
44 samples
40 predictors

No pre-processing
Resampling: Cross-Validated (4 fold, repeated 100 times) 
Summary of sample sizes: 36, 32, 32, 32, 32, 34, ... 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE     
   1    11689.06  0.2651247  8908.478
   2    11655.76  0.2566566  8618.057  <----- mtry selected
   3    11745.18  0.2469778  8608.479
   4    11833.94  0.2395274  8642.848
   5    11867.03  0.2378348  8651.408
   6    11922.31  0.2341872  8676.339
   7    11961.21  0.2322326  8696.816
   8    11985.05  0.2308759  8697.349
   9    12015.77  0.2289175  8716.417
  10    12033.04  0.2289433  8722.395
  11    12056.17  0.2270848  8739.421
  12    12075.65  0.2257823  8747.684
  13    12086.47  0.2259476  8748.066
  14    12091.98  0.2259209  8751.193
  15    12103.82  0.2260043  8761.587
  16    12125.92  0.2241327  8770.015
  17    12125.14  0.2242886  8766.597
  18    12129.33  0.2247183  8779.543
  19    12129.76  0.2252407  8778.346
  20    12144.24  0.2238568  8784.614

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 2.

predictions_rf
ID           2        6        7       15       16       17       23       24
Kg_He 37385.09 50909.30 22253.45 48546.61 47664.51 42782.99 32440.54 31270.95

ID          27       31       32       35       36       37       38       39
Kg_He 48215.34 34216.91 23951.96 37308.29 36987.70 50509.58 51328.06 44155.46
```
![RMS vs Ramdom Predictors Random Forest model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/RMSE_vs_Ramdom_Predictors_rforest.png?raw=true "RMS vs Ramdom Predictors Random Forest model")


Conditional Random Forests hyperparameters:
```r
44 samples
40 predictors

No pre-processing
Resampling: Cross-Validated (4 fold, repeated 100 times) 
Summary of sample sizes: 36, 32, 32, 32, 32, 34, ... 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE      
   1    12465.23  0.2353040  10071.399
   2    12252.20  0.2375762   9869.903
   3    12121.59  0.2374923   9722.160
   4    12051.66  0.2374132   9624.976
   5    12009.50  0.2364427   9567.536
   6    11989.49  0.2316047   9530.515
   7    11960.31  0.2352468   9480.496
   8    11957.48  0.2336638   9463.725
   9    11952.72  0.2303900   9447.680  <----- mtry selected
  10    11956.96  0.2311181   9435.015
  11    11965.15  0.2287326   9433.157
  12    11959.92  0.2289213   9414.927
  13    11965.28  0.2284617   9409.015
  14    11971.83  0.2267030   9402.886
  15    11973.15  0.2250927   9397.044
  16    11977.69  0.2255073   9387.420
  17    11976.50  0.2250848   9381.885
  18    11986.95  0.2235273   9385.585
  19    12003.37  0.2197853   9392.697
  20    11995.87  0.2226220   9376.090

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.

predictions_crf
ID           2        6        7       15       16       17       23       24
Kg_He 39859.23 46642.12 31497.72 48518.40 47799.10 40757.52 34521.58 34494.90

ID          27       31       32       35       36       37       38       39
Kg_He 42669.19 34922.35 33042.21 35879.30 35808.38 48648.83 48876.35 44926.69
```
![RMS vs Ramdom Predictors Conditional Inference Forest model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/RMSE_vs_Ramdom_Predictors_cforest.png?raw=true "RMS vs Ramdom Predictors Conditional Inference Forest model")

### Plot of variable importance
Something important when looking at the generated model, is variable importance, which is possible thanks to Caret's "varImp" function.

- Random Forest model:
![Variable importance ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_rforest_ggplo2.png?raw=true "Variable importance ML model")

- Conditional Random Forests model:
![Variable importance ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_cforest_ggplo2.png?raw=true "Variable importance ML model")

#### We have also calculate Feature Importance for each Variable vs Dropout Loss.

- Random Forest model:
![Variable importance boxplot ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_boxplot_rforest_ggplo2.png?raw=true "Variable importance boxplot ML model")

- Conditional Random Forests model:
![Variable importance boxplot ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_boxplot_crforest_ggplo2.png?raw=true "Variable importance boxplot ML model")

#### We have also calculate Feature Importance by each year used in the model.

- Random Forest model:
![Variable importance boxplot ML model by year](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_boxplot_rforest_by_year_ggplo2.png?raw=true "Variable importance boxplot ML model by year")

- Conditional Random Forests model:
![Variable importance boxplot ML model by year](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_boxplot_crforest_by_year_ggplo2.png?raw=true "Variable importance boxplot ML model by year")

### Values of R2, RMSE & MAE
These are the values with the best performance, after the auto-selection of the tuning through the use of the Caret package.

- Random Forest model:

| `R2` | `RMSE Kg` | `MAE Kg` |
| :-------: | :------: | :-----: |
| 0.9240436 | 4668.0454024 | 2968.0485040 |

- Conditional Random Forests model:

| `R2` | `RMSE Kg` | `MAE Kg` |
| :-------: | :------: | :-----: |
| 0.7507679 | 8077.7103351 | 5462.6394075 |

### Model comparison

To make this comparison, two functions of the Caret package were used:

1. A resamples of the train generated by each model was carried out.
2. A boxplot of these was generated by analyzing their Rsquared.

- Rsquared's Bloxplot:

![Rsquared's Bloxplot](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/RF_vs_cRF_Boxplot_Rsquared.png?raw=true "Rsquared's Bloxplot")

---

## Results: Prediction Yields

Finally the predictions obtained for each model are as follows.

### Random Forest model:

| `Variety`	| `Field`	| `Year`	| `Area_He`	| `Kg_He` |
| :-------: | :---------: | :---: | :---: | :--------------: |
| THOMPSON	| THOMPSON 00	| 2021	| 9.6	| 41357.80 |
| THOMPSON	| THOMPSON 07	| 2021	| 3.4	| 36689.45 |
| THOMPSON	| THOMPSON 08	| 2021	| 3.4	| 35831.95 |
| THOMPSON	| THOMPSON 96	| 2021	| 9.5	| 35798.07 |
| CRIMSON	| CRIMSON  99-00	| 2021	| 12.2	| 45891.00 |
| CRIMSON	| CRIMSON 04	| 2021	| 2.4	| 47082.26 |
| ARRA	| ARRA 15 CR	| 2021	| 2.9	| 44344.63 |
| TIMCO	| TIMCO 14 CR	| 2021	| 4.1	| 43648.81 |
| INIAGRAPEONE	| INIAGRAPE ONE 14	| 2021	| 1.1	| 42528.35 |
| INIAGRAPEONE	| INIAGRAPE ONE 15	| 2021	| 3	| 42693.73 |
| SABLE	| SABLE 14	| 2021	| 4.2	| 36598.97 |
| SABLE	| SABLE 16	| 2021	| 1.9	| 40690.00 |

### Conditional Random Forests model:

| `Variety`	| `Field`	| `Year`	| `Area_He`	| `Kg_He` |
| :-------: | :---------: | :---: | :---: | :--------------: |
| THOMPSON	| THOMPSON 00	| 2021	| 9.6	| 40630.35 |
| THOMPSON	| THOMPSON 07	| 2021	| 3.4	| 35272.61 |
| THOMPSON	| THOMPSON 08	| 2021	| 3.4	| 35009.36 |
| THOMPSON	| THOMPSON 96	| 2021	| 9.5	| 34895.34 |
| CRIMSON	| CRIMSON  99-00	| 2021	| 12.2	| 44551.80 |
| CRIMSON	| CRIMSON 04	| 2021	| 2.4	| 45011.13 |
| ARRA	| ARRA 15 CR	| 2021	| 2.9	| 42912.32 |
| TIMCO	| TIMCO 14 CR	| 2021	| 4.1	| 42191.32 |
| INIAGRAPEONE	| INIAGRAPE ONE 14	| 2021	| 1.1	| 41554.74 |
| INIAGRAPEONE	| INIAGRAPE ONE 15	| 2021	| 3	| 41840.73 |
| SABLE	| SABLE 14	| 2021	| 4.2	| 35848.95 |
| SABLE	| SABLE 16	| 2021	| 1.9	| 43400.77 |

---

### Bibliography
- Gitelson, A.A.; Kaufman, Y.J.; Merzlyak, M.N. Use of a green channel in remote sensing of global vegetation from EOS-MODIS. Remote Sens. Environ. 1996, 58, 289–298.

- Xia, Rong, "Comparison of Random Forests and Cforest: Variable Importance Measures and Prediction Accuracies" (2009). All Graduate Plan B and other Reports. 1255. https://digitalcommons.usu.edu/gradreports/1255

- Crane-Droesch, Andrew. (2018). Machine learning methods for crop yield prediction and climate change impact assessment in agriculture. Environmental Research Letters. 13. 10.1088/1748-9326/aae159. 

- Braga, Dieinison & Coelho da Silva, Ticiana & Rocha, Atslands & Coutinho do Rêgo, Luís Gustavo & Pires Magalhaes, Regis & Guerra, Paulo. (2020). Time Series Forecasting to Support Irrigation Management. Journal on Data Semantics. 10. 66-80. 

- Zhao, Yan & Potgieter, Andries & Zhang, Miao & Wu, Bingfang & Hammer, G.. (2020). remote sensing Predicting Wheat Yield at the Field Scale by Combining High-Resolution Sentinel-2 Satellite Imagery and Crop Modelling. Remote Sensing. 12. 10.3390/rs12061024. 

- Pantazi, X.E. & Moshou, Dimitrios & Alexandridis, Thomas & Whetton, Rebecca & Mouazen, Abdul. (2016). Wheat yield prediction using machine learning and advanced sensing techniques. Computers and Electronics in Agriculture. 121. 57-65. 10.1016/j.compag.2015.11.018. 

- Al-Gaadi, K.A. & Hassaballa, Abdalhaleem A. & Tola, Elkamil & Kayad, Ahmed & Madugundu, Rangaswamy & Alblewi, Bander & Assiri, Fahad. (2016). Prediction of Potato Crop Yield Using Precision Agriculture Techniques. PloS one. 11. e0162219. 10.1371/journal.pone.0162219. 

- Breiman L (2001). Random Forests. Machine Learning, 45(1), 5–32.

- Hothorn T, Hornik K, Zeileis A (2006b). Unbiased Recursive Partitioning: A Conditional Inference Framework. Journal of Computational and Graphical Statistics, 15(3), 651–674.

- Kuhn, M. (2008). Building Predictive Models in R Using the caret Package. Journal of Statistical Software, 28(5), 1 - 26. doi:http://dx.doi.org/10.18637/jss.v028.i05

- Wickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, https://ggplot2.tidyverse.org.

- Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686

---

## Presentation

If you are interested in seeing the presentation that was prepared for this project, you can download it at the following link:

- [Presentation](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/raw/main/Presentation_Inno_Lab_DAAD.pptx "Presentation")

---

#### *GNU General Public License v3.0 - Copyright (C)*

This script was made for testing purposes and may be used and modified in the future by those who see fit.