# [Innovation_laboratory_EAGLE](http://eagle-science.org/project/tmt1-innovation-laboratory/)
The innovation laboratory provides the opportunity to conduct a specified research project on a chosen research topic and explore the potential, challenges and limits of Earth Observation and geoanalysis in a practical approach during the 3rd term.
The Innovation Laboratory allows you to analyze one particular topic in your field of interest in depth. It allows to address own research in the field of the study program and offers the basis of practical attained knowledge. It is similar to an internship but allows to do it at the university (esp. our department).

---

## Content
The content of the innovation laboratory can be decided by each student individually and either a research topic is offered by a lecturer or the student is proposing an own topic. Research topics need to be discussed with and proposed to one EAGLE lecturer who will also be in charge of supervising and grading the students work. Topics of the innovation laboratory can cover all aspects of the EAGLE study program with a strong focus on applied Earth Observation and geoanalysis or its innovative potential for remote sensing sciences. It may comprise topics such as linking spectrometer field studies to remotely sensed data or the exploration of UAV based imagery, as well as space borne earth observation analysis such as time-series derivation for a variety of environmental studies i.e. resource mapping or spatial predictions and statistics of variables.

---

## Topic:
### Machine Learning process for yield prediction for the "Carmen Rosa" farm located in Chile.

Project carried out with the help of the company Carmen Rosa, who has provided data, both for its weather station and its annual production, in order to carry out an analysis and possible production optimization considering:

- Temperature
- Evapotranspiration
- Humidity
- Annual production
- Shapefile with the precise location of the crops

Image of Carmen Rosa Farm:
![Carmen Rosa farm location](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Carmen_Rosa_Farm.png?raw=true "Carmen Rosa farm location")

#### For better precision, check the location by downloading the following kmz:
 - [Kmz of Carmen Rosa Farm](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/raw/main/Original_data/CARMEN_ROSA_farm.kmz)

---

## Google Earth Engine
To carry out this project, an algorithm was generated through Google Earth Engine to obtain the images between November and December from 2016 to 2019, which represent the most important phenological stage for these grapes in this area of Chile. In the algorithm the images of GNDVI, NDVI and EVI were generated, which give us information about the crops and their health. Within the same algorithm, a mean of all the images for all their bands was obtained, which were downloaded and processed by the algorithms generated in R.
To obtain the information for the prediction of the year 2021, this algorithm was also used to obtain the desired images and bands.

The codes are as follows:
[Sentinel-2](https://code.earthengine.google.com/10bef8017a4fcacec6ef47296e2b9018)

---

## R Code generated
Then a code was applied to obtain a unique value per field for each band generated by the Google Earth Engine code and for each year, which will be applied in a machine learning model.

The codes are as follows:
[R code](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/NDVI_GNDVI_EVI_cellStats_by_Field.R)

---

## R Code boxplot
In order to get a general idea of each field, an NDVI analysis was performed for each field. For this, an analysis was performed through a boxplot of each NDVI image of each field and a point was placed that represents the mean of the total value of pixels of each image for each year.

The codes are as follows:
[R Boxplot code](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Boxplot.R)

#### Considerations when reading the graph:
For the fields 'INIAGRAPE ONE 15' and 'SABLE 16', only the years 2018 - 2019 are valid, since they did not present production in the years 2016 - 2017. This data was eliminated from the dataframe to create the machine learning model.

#### Boxplot:
![Example of NDVI for each field from the years 2016 -2019](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/NDVI_2017_to_2020.png?raw=true "Example of NDVI for each field from the years 2016 -2019")

## R Code [Caret Package](https://topepo.github.io/caret/index.html)
Applying the [Caret](https://topepo.github.io/caret/index.html) de R package, a [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning) model was made to make a prediction of the Yield of Carmen Rosa farm.
According to what has been done, the need for more data to make a prediction with better precision is evident. But considering the lack of these, it was decided to make (anyway) an [ML](https://en.wikipedia.org/wiki/Machine_learning) model, considering that it will improve over the years as more information is available.

### Evaluation and selection of Hyperparameters

When analyzing the values for tuning in the [Random Forest](https://en.wikipedia.org/wiki/Random_forest#:~:text=Random%20forests%20or%20random%20decision,average%20prediction%20(regression)%20of%20the) model and in the Conditional Random Forest model, the Caret package was used, which provides an analysis of the values and search for the best one for the model evaluation and for a better application. results.

Random Forest hyperparameters:
```r
44 samples
42 predictors

No pre-processing
Resampling: Cross-Validated (4 fold, repeated 100 times) 
Summary of sample sizes: 36, 32, 32, 32, 32, 34, ... 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE     
   1    11676.83  0.2693490  8890.271
   2    11610.96  0.2621742  8567.306
   3    11689.41  0.2532660  8545.084
   4    11774.60  0.2456405  8566.194
   5    11829.79  0.2412606  8588.528
   6    11875.31  0.2389845  8612.784
   7    11894.02  0.2387554  8619.813
   8    11929.05  0.2371073  8638.596
   9    11955.82  0.2346754  8646.653
  10    11976.27  0.2335992  8662.602
  11    11998.81  0.2320998  8673.819
  12    12021.09  0.2316459  8690.126
  13    12034.43  0.2305717  8695.754
  14    12043.21  0.2298851  8710.435
  15    12057.85  0.2295032  8714.386
  16    12071.08  0.2287177  8724.016
  17    12075.13  0.2281054  8723.732
  18    12083.63  0.2279444  8728.356
  19    12095.98  0.2277961  8746.239
  20    12095.40  0.2269736  8743.080

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 2.

predictions_rf
       2        6        7       15       16       23       24       27       31       35 
37590.76 50693.10 23392.90 48466.79 47515.99 32318.44 31222.99 48384.71 33659.60 37503.35 
      38       39 
51162.52 44316.09 
```
![RMS vs Ramdom Predictors Random Forest model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/RMSE_vs_Ramdom_Predictors_rforest.png?raw=true "RMS vs Ramdom Predictors Random Forest model")


Conditional Random Forest hyperparameters:
```r
44 samples
42 predictors

No pre-processing
Resampling: Cross-Validated (4 fold, repeated 100 times) 
Summary of sample sizes: 36, 32, 32, 32, 32, 34, ... 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE      
   1    12486.20  0.2296676  10084.008
   2    12272.32  0.2353181   9880.017
   3    12156.38  0.2325102   9744.929
   4    12079.01  0.2327008   9648.538
   5    12035.74  0.2313972   9580.441
   6    12004.96  0.2311830   9535.947
   7    11996.16  0.2289766   9512.078
   8    11984.79  0.2288810   9487.270
   9    11978.02  0.2284915   9465.462
  10    11977.00  0.2275237   9450.583
  11    11977.48  0.2262284   9439.691
  12    11979.42  0.2258908   9432.252
  13    11991.62  0.2239698   9432.875
  14    11990.17  0.2226403   9419.119
  15    11983.13  0.2249086   9407.877
  16    11987.69  0.2238611   9403.176
  17    11992.29  0.2229104   9401.631
  18    12001.19  0.2214549   9396.781
  19    11999.20  0.2215391   9385.345
  20    12010.37  0.2202692   9390.855

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 10.

redictions_crf
 [1] 39592.88 46064.02 31695.47 48051.45 47635.13 34848.37 34712.48 42283.37 35127.84 36431.38
[11] 48565.71 44912.85
```
![RMS vs Ramdom Predictors Conditional Random Forest model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/RMSE_vs_Ramdom_Predictors_cforest.png?raw=true "RMS vs Ramdom Predictors Conditional Random Forest model")

### Plot of variable importance
Something important when looking at the generated model, is variable importance, which is possible thanks to Caret's "varImp" function.

- RandomForest model:
![Variable importance ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_rforest_ggplo2.png?raw=true "Variable importance ML model")

- Conditional RandomForest model:
![Variable importance ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_cforest_ggplo2.png?raw=true "Variable importance ML model")

We have also calculate Feature Importance Explanations As Loss From Feature Dropout.

- RandomForest model:
![Variable importance boxplot ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_boxplot_rforest_ggplo2.png?raw=true "Variable importance boxplot ML model")

- Conditional RandomForest model:
![Variable importance boxplot ML model](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/Variable_Importance_boxplot_crforest_ggplo2.png?raw=true "Variable importance boxplot ML model")

### Values of R2, RMSE & MAE
These are the values with the best performance, after the auto-selection of the tuning through the use of the Caret package.

- RandomForest model:

| `R2` | `RMSE Kg` | `MAE Kg` |
| :-------: | :------: | :-----: |
| 0.9490709 | 3037.2625979 | 2430.5169700 |

- Conditional RandomForest model:

| `R2` | `RMSE Kg` | `MAE Kg` |
| :-------: | :------: | :-----: |
| 0.8061893 | 6121.8310575 | 4740.1615328 |

### Model comparison

To make this comparison, two functions of the Caret package were used:

1. A resamples of the train generated by each model was carried out.
2. A boxplot of these was generated by analyzing their Rsquared.

- Rsquared's Bloxplot:

![Rsquared's Bloxplot](https://github.com/diegoalarc/Innovation_laboratory_EAGLE/blob/main/Plots/RF_vs_cRF_Boxplot_Rsquared.png?raw=true "Rsquared's Bloxplot")